2025-09-03 17:25:38,422 - __main__ - INFO - ==================================================
2025-09-03 17:25:38,422 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:25:38,422 - __main__ - INFO - Timestamp: 2025-09-03 17:25:38.422685
2025-09-03 17:25:38,422 - __main__ - INFO - ==================================================
2025-09-03 17:25:39,138 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:25:39,138 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:25:39,141 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:25:39,141 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:25:39,141 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:25:39,145 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:25:39,145 - utils - INFO - Starting daily market summary generation
2025-09-03 17:25:39,145 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:25:39,145 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:25:39,145 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:25:39,145 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:25:39,146 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:25:39,146 - utils - ERROR - Error in daily summary workflow: 3 validation errors for Task
context.list[function-after[handle_max_retries_deprecation(), function-after[check_output(), function-after[check_tools(), function-after[set_attributes_based_on_config(), function-after[ensure_guardrail_is_callable(), function-after[validate_required_fields(), Task]]]]]]].0.description
  Field required [type=missing, input_value={'formatted': Task(descri...lication
            )]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
context.list[function-after[handle_max_retries_deprecation(), function-after[check_output(), function-after[check_tools(), function-after[set_attributes_based_on_config(), function-after[ensure_guardrail_is_callable(), function-after[validate_required_fields(), Task]]]]]]].0.expected_output
  Field required [type=missing, input_value={'formatted': Task(descri...lication
            )]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
context.is-instance[_NotSpecified]
  Input should be an instance of _NotSpecified [type=is_instance_of, input_value=[{'formatted': Task(descr...ication
            )]}], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
2025-09-03 17:25:39,146 - __main__ - ERROR - Market summary generation failed: 3 validation errors for Task
context.list[function-after[handle_max_retries_deprecation(), function-after[check_output(), function-after[check_tools(), function-after[set_attributes_based_on_config(), function-after[ensure_guardrail_is_callable(), function-after[validate_required_fields(), Task]]]]]]].0.description
  Field required [type=missing, input_value={'formatted': Task(descri...lication
            )]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
context.list[function-after[handle_max_retries_deprecation(), function-after[check_output(), function-after[check_tools(), function-after[set_attributes_based_on_config(), function-after[ensure_guardrail_is_callable(), function-after[validate_required_fields(), Task]]]]]]].0.expected_output
  Field required [type=missing, input_value={'formatted': Task(descri...lication
            )]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
context.is-instance[_NotSpecified]
  Input should be an instance of _NotSpecified [type=is_instance_of, input_value=[{'formatted': Task(descr...ication
            )]}], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
2025-09-03 17:25:39,146 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 99, in run_daily_summary
    send_task = self.tasks.create_send_task(self.send_agent, {
        'formatted': formatting_task,
        'translations': translation_tasks
    })
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\tasks.py", line 181, in create_send_task
    return Task(
        description=f"""
    ...<39 lines>...
        output_file="delivery_report.json"
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 3 validation errors for Task
context.list[function-after[handle_max_retries_deprecation(), function-after[check_output(), function-after[check_tools(), function-after[set_attributes_based_on_config(), function-after[ensure_guardrail_is_callable(), function-after[validate_required_fields(), Task]]]]]]].0.description
  Field required [type=missing, input_value={'formatted': Task(descri...lication
            )]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
context.list[function-after[handle_max_retries_deprecation(), function-after[check_output(), function-after[check_tools(), function-after[set_attributes_based_on_config(), function-after[ensure_guardrail_is_callable(), function-after[validate_required_fields(), Task]]]]]]].0.expected_output
  Field required [type=missing, input_value={'formatted': Task(descri...lication
            )]}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/missing
context.is-instance[_NotSpecified]
  Input should be an instance of _NotSpecified [type=is_instance_of, input_value=[{'formatted': Task(descr...ication
            )]}], input_type=list]
    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of
2025-09-03 17:27:22,178 - __main__ - INFO - ==================================================
2025-09-03 17:27:22,179 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:27:22,179 - __main__ - INFO - Timestamp: 2025-09-03 17:27:22.179371
2025-09-03 17:27:22,179 - __main__ - INFO - ==================================================
2025-09-03 17:27:22,620 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:27:22,621 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:27:22,621 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:27:22,621 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:27:22,621 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:27:22,625 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:27:22,626 - utils - INFO - Starting daily market summary generation
2025-09-03 17:27:22,626 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:27:22,626 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:27:22,626 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:27:22,627 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:27:22,627 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:27:22,659 - utils - ERROR - Error in daily summary workflow: 1 validation error for Crew
  Value error, The CHROMA_OPENAI_API_KEY environment variable is not set. [type=value_error, input_value={'agents': [Agent(role=Fi...xt-embedding-3-small'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
2025-09-03 17:27:22,660 - __main__ - ERROR - Market summary generation failed: 1 validation error for Crew
  Value error, The CHROMA_OPENAI_API_KEY environment variable is not set. [type=value_error, input_value={'agents': [Agent(role=Fi...xt-embedding-3-small'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
2025-09-03 17:27:22,660 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 107, in run_daily_summary
    crew = self.create_crew(tasks_list)
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 46, in create_crew
    return Crew(
        agents=[
    ...<16 lines>...
        }
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for Crew
  Value error, The CHROMA_OPENAI_API_KEY environment variable is not set. [type=value_error, input_value={'agents': [Agent(role=Fi...xt-embedding-3-small'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
2025-09-03 17:30:52,609 - __main__ - INFO - ==================================================
2025-09-03 17:30:52,609 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:30:52,610 - __main__ - INFO - Timestamp: 2025-09-03 17:30:52.610031
2025-09-03 17:30:52,610 - __main__ - INFO - ==================================================
2025-09-03 17:30:53,036 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:30:53,036 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:30:53,036 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:30:53,036 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:30:53,037 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:30:53,040 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:30:53,040 - utils - INFO - Starting daily market summary generation
2025-09-03 17:30:53,040 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:30:53,041 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:30:53,041 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:30:53,041 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:30:53,042 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:30:53,253 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:30:55,912 - root - INFO - Collection found or created: Collection(name=short_term)
2025-09-03 17:30:56,069 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:30:56,382 - root - INFO - Collection found or created: Collection(name=entities)
2025-09-03 17:30:56,382 - utils - ERROR - Error in daily summary workflow: 1 validation error for Crew
  Value error, Tool is not a CrewStructuredTool or BaseTool [type=value_error, input_value={'agents': [Agent(role=Fi...xt-embedding-3-small'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
2025-09-03 17:30:56,383 - __main__ - ERROR - Market summary generation failed: 1 validation error for Crew
  Value error, Tool is not a CrewStructuredTool or BaseTool [type=value_error, input_value={'agents': [Agent(role=Fi...xt-embedding-3-small'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
2025-09-03 17:30:56,383 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 113, in run_daily_summary
    crew = self.create_crew(tasks_list)
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 52, in create_crew
    return Crew(
        agents=[
    ...<16 lines>...
        }
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for Crew
  Value error, Tool is not a CrewStructuredTool or BaseTool [type=value_error, input_value={'agents': [Agent(role=Fi...xt-embedding-3-small'}}}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.11/v/value_error
2025-09-03 17:32:21,591 - __main__ - INFO - ==================================================
2025-09-03 17:32:21,591 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:32:21,591 - __main__ - INFO - Timestamp: 2025-09-03 17:32:21.591851
2025-09-03 17:32:21,591 - __main__ - INFO - ==================================================
2025-09-03 17:32:22,053 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:32:22,053 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:32:22,053 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:32:22,053 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:32:22,054 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:32:22,058 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:32:22,058 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:32:22,058 - utils - INFO - Starting daily market summary generation
2025-09-03 17:32:22,058 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:32:22,058 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:32:22,058 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:32:22,059 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:32:22,059 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:32:22,229 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:32:22,452 - root - INFO - Collection found or created: Collection(name=short_term)
2025-09-03 17:32:22,608 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:32:22,791 - root - INFO - Collection found or created: Collection(name=entities)
2025-09-03 17:32:22,794 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:32:22,824 - LiteLLM - INFO - 
LiteLLM completion() model= gpt-4o-mini; provider = openai
2025-09-03 17:32:23,458 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 401 Unauthorized"
2025-09-03 17:32:23,602 - utils - ERROR - Error in daily summary workflow: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: sk-proj-*****************************e.... You can find your API key at https://platform.openai.com/account/api-keys.
2025-09-03 17:32:23,603 - __main__ - ERROR - Market summary generation failed: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: sk-proj-*****************************e.... You can find your API key at https://platform.openai.com/account/api-keys.
2025-09-03 17:32:23,603 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 725, in completion
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 653, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        openai_client=openai_client,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        logging_obj=logging_obj,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 149, in sync_wrapper
    result = func(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 471, in make_sync_openai_chat_completion_request
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 453, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
        **data, timeout=timeout
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1147, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*****************************e.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1966, in completion
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1939, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 736, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-proj-*****************************e.... You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 120, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 658, in kickoff
    self._handle_crew_planning()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 736, in _handle_crew_planning
    )._handle_crew_planning()
      ~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\planning_handler.py", line 46, in _handle_crew_planning
    result = planner_task.execute_sync()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 456, in exception_type
    raise AuthenticationError(
    ...<5 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: OpenAIException - Incorrect API key provided: sk-proj-*****************************e.... You can find your API key at https://platform.openai.com/account/api-keys.
2025-09-03 17:37:31,381 - __main__ - INFO - ==================================================
2025-09-03 17:37:31,381 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:37:31,382 - __main__ - INFO - Timestamp: 2025-09-03 17:37:31.382125
2025-09-03 17:37:31,382 - __main__ - INFO - ==================================================
2025-09-03 17:37:31,814 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:37:31,814 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:37:31,814 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:37:31,815 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:37:31,815 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:37:31,818 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:37:31,819 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:37:31,819 - utils - INFO - Starting daily market summary generation
2025-09-03 17:37:31,819 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:37:31,819 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:37:31,820 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:37:31,820 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:37:31,820 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:37:31,988 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:37:32,214 - root - INFO - Collection found or created: Collection(name=short_term)
2025-09-03 17:37:32,371 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:37:32,553 - root - INFO - Collection found or created: Collection(name=entities)
2025-09-03 17:37:32,556 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:37:32,589 - LiteLLM - INFO - 
LiteLLM completion() model= gpt-4o-mini; provider = openai
2025-09-03 17:37:34,767 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 17:37:34,768 - openai._base_client - INFO - Retrying request to /chat/completions in 0.385708 seconds
2025-09-03 17:37:36,130 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 17:37:36,132 - openai._base_client - INFO - Retrying request to /chat/completions in 0.770048 seconds
2025-09-03 17:37:38,058 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 17:37:38,092 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.
2025-09-03 17:37:38,092 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.
2025-09-03 17:37:38,092 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 725, in completion
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 653, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        openai_client=openai_client,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        logging_obj=logging_obj,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 149, in sync_wrapper
    result = func(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 471, in make_sync_openai_chat_completion_request
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 453, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
        **data, timeout=timeout
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1147, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1966, in completion
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1939, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 736, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 120, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 658, in kickoff
    self._handle_crew_planning()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 736, in _handle_crew_planning
    )._handle_crew_planning()
      ~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\planning_handler.py", line 46, in _handle_crew_planning
    result = planner_task.execute_sync()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.
2025-09-03 17:49:16,478 - __main__ - INFO - ==================================================
2025-09-03 17:49:16,478 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:49:16,478 - __main__ - INFO - Timestamp: 2025-09-03 17:49:16.478677
2025-09-03 17:49:16,478 - __main__ - INFO - ==================================================
2025-09-03 17:49:16,953 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:49:16,953 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:49:16,953 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:49:16,953 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:49:16,953 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:49:16,958 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:49:16,958 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:49:16,958 - utils - INFO - Starting daily market summary generation
2025-09-03 17:49:16,958 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:49:16,959 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:49:16,959 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:49:16,959 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:49:16,960 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:49:17,132 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:49:17,346 - root - INFO - Collection found or created: Collection(name=short_term)
2025-09-03 17:49:17,502 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
2025-09-03 17:49:17,693 - root - INFO - Collection found or created: Collection(name=entities)
2025-09-03 17:49:17,696 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:49:17,733 - LiteLLM - INFO - 
LiteLLM completion() model= gpt-4o-mini; provider = openai
2025-09-03 17:49:18,467 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 17:49:18,468 - openai._base_client - INFO - Retrying request to /chat/completions in 0.411929 seconds
2025-09-03 17:49:19,570 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 17:49:19,570 - openai._base_client - INFO - Retrying request to /chat/completions in 0.913744 seconds
2025-09-03 17:49:20,809 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 17:49:20,835 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.
2025-09-03 17:49:20,835 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.
2025-09-03 17:49:20,835 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 725, in completion
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 653, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        openai_client=openai_client,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        logging_obj=logging_obj,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 149, in sync_wrapper
    result = func(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 471, in make_sync_openai_chat_completion_request
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 453, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
        **data, timeout=timeout
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\resources\chat\completions\completions.py", line 1147, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\openai\_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1966, in completion
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1939, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\openai\openai.py", line 736, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 120, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 658, in kickoff
    self._handle_crew_planning()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 736, in _handle_crew_planning
    )._handle_crew_planning()
      ~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\planning_handler.py", line 46, in _handle_crew_planning
    result = planner_task.execute_sync()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: OpenAIException - You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.
2025-09-03 17:52:33,349 - __main__ - INFO - ==================================================
2025-09-03 17:52:33,349 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:52:33,349 - __main__ - INFO - Timestamp: 2025-09-03 17:52:33.349617
2025-09-03 17:52:33,349 - __main__ - INFO - ==================================================
2025-09-03 17:52:33,711 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:52:33,711 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:52:33,711 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:52:33,711 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:52:33,712 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:52:33,716 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:52:33,716 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:52:33,716 - utils - INFO - Starting daily market summary generation
2025-09-03 17:52:33,717 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:52:33,717 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:52:33,717 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:52:33,717 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:52:33,718 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:52:33,723 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:52:33,775 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.1-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-03 17:52:33,775 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.1-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-03 17:52:33,775 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 123, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.1-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-03 17:53:24,192 - __main__ - INFO - ==================================================
2025-09-03 17:53:24,193 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:53:24,193 - __main__ - INFO - Timestamp: 2025-09-03 17:53:24.193165
2025-09-03 17:53:24,193 - __main__ - INFO - ==================================================
2025-09-03 17:53:24,557 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:53:24,557 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:53:24,557 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:53:24,557 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:53:24,557 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:53:24,562 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:53:24,562 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:53:24,562 - utils - INFO - Starting daily market summary generation
2025-09-03 17:53:24,562 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:53:24,562 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:53:24,562 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:53:24,563 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:53:24,563 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:53:24,568 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:53:24,611 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.1-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-03 17:53:24,611 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.1-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-03 17:53:24,611 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 123, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.1-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-03 17:55:43,425 - __main__ - INFO - ==================================================
2025-09-03 17:55:43,425 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:55:43,425 - __main__ - INFO - Timestamp: 2025-09-03 17:55:43.425789
2025-09-03 17:55:43,425 - __main__ - INFO - ==================================================
2025-09-03 17:55:43,426 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:55:43,426 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:55:43,426 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:55:43,426 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:55:43,426 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:55:43,429 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:55:43,429 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:55:43,429 - utils - INFO - Starting daily market summary generation
2025-09-03 17:55:43,429 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:55:43,429 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:55:43,430 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:55:43,430 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:55:43,430 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:55:43,434 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:55:43,466 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.1-70b-versatile; provider = groq
2025-09-03 17:55:43,766 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-03 17:55:43,822 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-03 17:55:43,822 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-03 17:55:43,822 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 123, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 391, in exception_type
    raise BadRequestError(
    ...<6 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-03 17:56:50,798 - __main__ - INFO - ==================================================
2025-09-03 17:56:50,798 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:56:50,799 - __main__ - INFO - Timestamp: 2025-09-03 17:56:50.799178
2025-09-03 17:56:50,799 - __main__ - INFO - ==================================================
2025-09-03 17:56:50,799 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:56:50,799 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:56:50,800 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:56:50,800 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:56:50,800 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:56:50,803 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:56:50,803 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:56:50,803 - utils - INFO - Starting daily market summary generation
2025-09-03 17:56:50,804 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:56:50,804 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:56:50,804 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:56:50,804 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:56:50,804 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:56:50,809 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:56:50,835 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:57:52,489 - __main__ - INFO - ==================================================
2025-09-03 17:57:52,490 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 17:57:52,490 - __main__ - INFO - Timestamp: 2025-09-03 17:57:52.490185
2025-09-03 17:57:52,490 - __main__ - INFO - ==================================================
2025-09-03 17:57:52,490 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 17:57:52,490 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 17:57:52,490 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 17:57:52,490 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 17:57:52,490 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 17:57:52,493 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 17:57:52,493 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 17:57:52,493 - utils - INFO - Starting daily market summary generation
2025-09-03 17:57:52,494 - utils - INFO - Step 1: Searching for financial news
2025-09-03 17:57:52,494 - utils - INFO - Step 2: Creating market summary
2025-09-03 17:57:52,494 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 17:57:52,494 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 17:57:52,494 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 17:57:52,498 - utils - INFO - Executing CrewAI workflow
2025-09-03 17:57:52,523 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:57:55,679 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 17:57:55,680 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 17:57:55,709 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:57:57,346 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 17:57:57,349 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 17:57:57,382 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:57:58,869 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 17:57:58,871 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 17:57:58,905 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:58:03,255 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 17:58:03,257 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 17:58:03,289 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:58:07,705 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 17:58:07,708 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 17:58:07,741 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:58:19,038 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 17:58:19,039 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 17:58:19,069 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 17:58:19,158 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 17:58:19,177 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9949, Requested 6003. Please try again in 19.757s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 17:58:19,178 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9949, Requested 6003. Please try again in 19.757s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 17:58:19,178 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9949, Requested 6003. Please try again in 19.757s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9949, Requested 6003. Please try again in 19.757s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:16:22,854 - __main__ - INFO - ==================================================
2025-09-03 18:16:22,854 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:16:22,854 - __main__ - INFO - Timestamp: 2025-09-03 18:16:22.854679
2025-09-03 18:16:22,854 - __main__ - INFO - ==================================================
2025-09-03 18:16:22,855 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:16:22,855 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:16:22,855 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:16:22,855 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:16:22,855 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:16:22,859 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:16:22,859 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:16:22,859 - utils - INFO - Starting daily market summary generation
2025-09-03 18:16:22,859 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:16:22,859 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:16:22,859 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:16:22,859 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:16:22,860 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:16:22,864 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:16:22,900 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:16:25,743 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:16:25,746 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:16:25,798 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:16:27,430 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:16:27,432 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:16:27,476 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:16:29,181 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:16:29,183 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:16:29,229 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:16:33,350 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:16:33,353 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:16:33,388 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:16:37,430 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:16:37,432 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:16:37,471 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:16:47,317 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:16:47,319 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:16:47,355 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:16:47,483 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:16:47,505 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10438, Requested 5872. Please try again in 21.546s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:16:47,506 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10438, Requested 5872. Please try again in 21.546s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:16:47,506 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10438, Requested 5872. Please try again in 21.546s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10438, Requested 5872. Please try again in 21.546s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:20:56,025 - __main__ - INFO - ==================================================
2025-09-03 18:20:56,026 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:20:56,026 - __main__ - INFO - Timestamp: 2025-09-03 18:20:56.026138
2025-09-03 18:20:56,026 - __main__ - INFO - ==================================================
2025-09-03 18:20:56,026 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:20:56,026 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:20:56,026 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:20:56,026 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:20:56,026 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:20:56,029 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:20:56,030 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:20:56,030 - utils - INFO - Starting daily market summary generation
2025-09-03 18:20:56,030 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:20:56,030 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:20:56,030 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:20:56,030 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:20:56,030 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:20:56,034 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:20:56,072 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:20:58,818 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:20:58,820 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:20:58,872 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:21:00,040 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:21:00,041 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:21:00,076 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:21:01,391 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:21:01,393 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:21:01,429 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:21:04,860 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:21:04,862 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:21:04,894 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:21:07,862 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:21:07,864 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:21:07,894 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:21:16,172 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:21:16,173 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:21:16,214 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:21:16,268 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:21:16,285 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10049, Requested 4907. Please try again in 14.779s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:21:16,285 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10049, Requested 4907. Please try again in 14.779s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:21:16,285 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10049, Requested 4907. Please try again in 14.779s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10049, Requested 4907. Please try again in 14.779s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:23:37,227 - __main__ - INFO - ==================================================
2025-09-03 18:23:37,227 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:23:37,227 - __main__ - INFO - Timestamp: 2025-09-03 18:23:37.227711
2025-09-03 18:23:37,227 - __main__ - INFO - ==================================================
2025-09-03 18:23:37,228 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:23:37,228 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:23:37,228 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:23:37,228 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:23:37,228 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:23:37,231 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:23:37,231 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:23:37,231 - utils - INFO - Starting daily market summary generation
2025-09-03 18:23:37,231 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:23:37,231 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:23:37,231 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:23:37,232 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:23:37,232 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:23:37,236 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:23:37,263 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:23:40,200 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:23:40,201 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:23:40,247 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:23:41,819 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:23:41,821 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:23:41,855 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:23:43,368 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:23:43,370 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:23:43,415 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:23:47,441 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:23:47,442 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:23:47,475 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:23:51,043 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:23:51,044 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:23:51,077 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:24:01,537 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:24:01,538 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:24:01,572 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:24:01,646 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:24:01,665 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10224, Requested 5768. Please try again in 19.958s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:24:01,665 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10224, Requested 5768. Please try again in 19.958s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:24:01,665 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10224, Requested 5768. Please try again in 19.958s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10224, Requested 5768. Please try again in 19.958s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:26:26,053 - __main__ - INFO - ==================================================
2025-09-03 18:26:26,053 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:26:26,053 - __main__ - INFO - Timestamp: 2025-09-03 18:26:26.053986
2025-09-03 18:26:26,054 - __main__ - INFO - ==================================================
2025-09-03 18:26:26,054 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:26:26,054 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:26:26,054 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:26:26,054 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:26:26,055 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:26:26,058 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:26:26,058 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:26:26,058 - utils - INFO - Starting daily market summary generation
2025-09-03 18:26:26,058 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:26:26,058 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:26:26,059 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:26:26,059 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:26:26,059 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:26:26,063 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:26:26,094 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:26:29,706 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:26:29,707 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:26:29,749 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:26:31,176 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:26:31,177 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:26:31,214 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:26:33,060 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:26:33,061 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:26:33,105 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:26:38,656 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:26:38,657 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:26:38,702 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:26:42,947 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:26:42,949 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:26:43,014 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:26:55,086 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:26:55,087 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:26:55,131 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:26:55,214 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:26:55,235 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10072, Requested 6378. Please try again in 22.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:26:55,235 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10072, Requested 6378. Please try again in 22.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:26:55,235 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10072, Requested 6378. Please try again in 22.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10072, Requested 6378. Please try again in 22.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:30:10,999 - __main__ - INFO - ==================================================
2025-09-03 18:30:10,999 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:30:10,999 - __main__ - INFO - Timestamp: 2025-09-03 18:30:10.999369
2025-09-03 18:30:10,999 - __main__ - INFO - ==================================================
2025-09-03 18:30:10,999 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:30:10,999 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:30:10,999 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:30:10,999 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:30:10,999 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:30:11,003 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:30:11,003 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:30:11,003 - utils - INFO - Starting daily market summary generation
2025-09-03 18:30:11,003 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:30:11,003 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:30:11,003 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:30:11,003 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:30:11,003 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:30:11,008 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:30:11,037 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:30:14,308 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:30:14,311 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:30:14,349 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:30:16,002 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:30:16,004 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:30:16,045 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:30:17,719 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:30:17,720 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:30:17,759 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:30:22,569 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:30:22,572 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:30:22,606 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:30:26,646 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:30:26,648 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:30:26,696 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:30:38,495 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:30:38,497 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:30:38,540 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:30:38,611 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:30:38,627 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10425, Requested 6065. Please try again in 22.447s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:30:38,627 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10425, Requested 6065. Please try again in 22.447s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:30:38,627 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10425, Requested 6065. Please try again in 22.447s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10425, Requested 6065. Please try again in 22.447s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:32:15,324 - __main__ - INFO - ==================================================
2025-09-03 18:32:15,324 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:32:15,325 - __main__ - INFO - Timestamp: 2025-09-03 18:32:15.325045
2025-09-03 18:32:15,325 - __main__ - INFO - ==================================================
2025-09-03 18:32:15,325 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:32:15,325 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:32:15,325 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:32:15,325 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:32:15,325 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:32:15,328 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:32:15,328 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:32:15,328 - utils - INFO - Starting daily market summary generation
2025-09-03 18:32:15,328 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:32:15,329 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:32:15,329 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:32:15,329 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:32:15,329 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:32:15,333 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:32:15,359 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:32:18,209 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:32:18,211 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:32:18,253 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:32:19,637 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:32:19,638 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:32:19,663 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:32:21,194 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:32:21,196 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:32:21,233 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:32:24,776 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:32:24,778 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:32:24,819 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:32:28,216 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:32:28,217 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:32:28,263 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:32:37,975 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-03 18:32:37,977 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-03 18:32:38,044 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:32:38,115 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:32:38,146 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101426, Requested 5629. Please try again in 1h41m36.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:32:38,146 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101426, Requested 5629. Please try again in 1h41m36.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:32:38,146 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101426, Requested 5629. Please try again in 1h41m36.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101426, Requested 5629. Please try again in 1h41m36.247s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:36:06,783 - __main__ - INFO - ==================================================
2025-09-03 18:36:06,784 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:36:06,784 - __main__ - INFO - Timestamp: 2025-09-03 18:36:06.784198
2025-09-03 18:36:06,784 - __main__ - INFO - ==================================================
2025-09-03 18:36:06,784 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:36:06,784 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:36:06,784 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:36:06,784 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:36:06,785 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:36:06,788 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:36:06,788 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:36:06,788 - utils - INFO - Starting daily market summary generation
2025-09-03 18:36:06,788 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:36:06,788 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:36:06,788 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:36:06,789 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:36:06,789 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:36:06,793 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:36:06,828 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:36:07,169 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:36:07,185 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101184, Requested 760. Please try again in 28m0.385s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:36:07,185 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101184, Requested 760. Please try again in 28m0.385s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:36:07,185 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101184, Requested 760. Please try again in 28m0.385s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101184, Requested 760. Please try again in 28m0.385s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:36:34,732 - __main__ - INFO - ==================================================
2025-09-03 18:36:34,733 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-03 18:36:34,733 - __main__ - INFO - Timestamp: 2025-09-03 18:36:34.733165
2025-09-03 18:36:34,733 - __main__ - INFO - ==================================================
2025-09-03 18:36:34,733 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-03 18:36:34,733 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-03 18:36:34,733 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-03 18:36:34,733 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-03 18:36:34,734 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-03 18:36:34,737 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-03 18:36:34,737 - utils - INFO - Market Summary Crew initialized successfully
2025-09-03 18:36:34,737 - utils - INFO - Starting daily market summary generation
2025-09-03 18:36:34,737 - utils - INFO - Step 1: Searching for financial news
2025-09-03 18:36:34,737 - utils - INFO - Step 2: Creating market summary
2025-09-03 18:36:34,737 - utils - INFO - Step 3: Formatting with visual elements
2025-09-03 18:36:34,738 - utils - INFO - Step 4: Translating to multiple languages
2025-09-03 18:36:34,738 - utils - INFO - Step 5: Sending to Telegram
2025-09-03 18:36:34,742 - utils - INFO - Executing CrewAI workflow
2025-09-03 18:36:34,770 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-03 18:36:35,005 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-03 18:36:35,023 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101152, Requested 760. Please try again in 27m32.539s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:36:35,023 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101152, Requested 760. Please try again in 27m32.539s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-03 18:36:35,024 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101152, Requested 760. Please try again in 27m32.539s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 101152, Requested 760. Please try again in 27m32.539s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:30:35,945 - __main__ - INFO - ==================================================
2025-09-04 07:30:35,945 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 07:30:35,945 - __main__ - INFO - Timestamp: 2025-09-04 07:30:35.945694
2025-09-04 07:30:35,945 - __main__ - INFO - ==================================================
2025-09-04 07:30:35,946 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 07:30:35,946 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 07:30:35,946 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 07:30:35,946 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 07:30:35,946 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 07:30:35,950 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-04 07:30:35,950 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 07:30:35,950 - utils - INFO - Starting daily market summary generation
2025-09-04 07:30:35,950 - utils - INFO - Step 1: Searching for financial news
2025-09-04 07:30:35,950 - utils - INFO - Step 2: Creating market summary
2025-09-04 07:30:35,950 - utils - INFO - Step 3: Formatting with visual elements
2025-09-04 07:30:35,950 - utils - INFO - Step 4: Translating to multiple languages
2025-09-04 07:30:35,951 - utils - INFO - Step 5: Sending to Telegram
2025-09-04 07:30:35,960 - utils - INFO - Executing CrewAI workflow
2025-09-04 07:30:35,986 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:30:38,785 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:30:38,786 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:30:38,841 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:30:40,161 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:30:40,161 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:30:40,209 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:30:41,482 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:30:41,483 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:30:41,541 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:30:44,962 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:30:44,964 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:30:45,016 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:30:48,188 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:30:48,190 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:30:48,239 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:30:56,725 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:30:56,727 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:30:56,780 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:30:56,856 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-04 07:30:56,944 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9576, Requested 4975. Please try again in 12.753s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:30:56,944 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9576, Requested 4975. Please try again in 12.753s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:30:56,944 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9576, Requested 4975. Please try again in 12.753s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9576, Requested 4975. Please try again in 12.753s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:36:09,010 - __main__ - INFO - ==================================================
2025-09-04 07:36:09,010 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 07:36:09,010 - __main__ - INFO - Timestamp: 2025-09-04 07:36:09.010700
2025-09-04 07:36:09,010 - __main__ - INFO - ==================================================
2025-09-04 07:36:09,011 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 07:36:09,011 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 07:36:09,011 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 07:36:09,011 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 07:36:09,011 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 07:36:09,014 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-04 07:36:09,015 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 07:36:09,015 - utils - INFO - Starting daily market summary generation
2025-09-04 07:36:09,015 - utils - INFO - Step 1: Searching for financial news
2025-09-04 07:36:09,015 - utils - INFO - Step 2: Creating market summary
2025-09-04 07:36:09,015 - utils - INFO - Step 3: Formatting with visual elements
2025-09-04 07:36:09,015 - utils - INFO - Step 4: Translating to multiple languages
2025-09-04 07:36:09,016 - utils - INFO - Step 5: Sending to Telegram
2025-09-04 07:36:09,019 - utils - INFO - Executing CrewAI workflow
2025-09-04 07:36:09,064 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:36:12,135 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:36:12,138 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:36:12,183 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:36:13,830 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:36:13,833 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:36:13,870 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:36:15,287 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:36:15,289 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:36:15,322 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:36:19,588 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:36:19,590 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:36:19,626 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:36:24,298 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:36:24,299 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:36:24,329 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:36:35,470 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:36:35,472 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:36:35,504 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:36:35,586 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-04 07:36:35,607 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9872, Requested 6029. Please try again in 19.505s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:36:35,607 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9872, Requested 6029. Please try again in 19.505s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:36:35,608 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9872, Requested 6029. Please try again in 19.505s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 9872, Requested 6029. Please try again in 19.505s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:42:15,474 - __main__ - INFO - ==================================================
2025-09-04 07:42:15,474 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 07:42:15,474 - __main__ - INFO - Timestamp: 2025-09-04 07:42:15.474383
2025-09-04 07:42:15,474 - __main__ - INFO - ==================================================
2025-09-04 07:42:15,474 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 07:42:15,474 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 07:42:15,475 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 07:42:15,475 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 07:42:15,475 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 07:42:15,478 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-04 07:42:15,478 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 07:42:15,478 - utils - INFO - Starting daily market summary generation
2025-09-04 07:42:15,479 - utils - INFO - Step 1: Searching for financial news
2025-09-04 07:42:15,479 - utils - INFO - Step 2: Creating market summary
2025-09-04 07:42:15,479 - utils - INFO - Step 3: Formatting with visual elements
2025-09-04 07:42:15,479 - utils - INFO - Step 4: Translating to multiple languages
2025-09-04 07:42:15,480 - utils - INFO - Step 5: Sending to Telegram
2025-09-04 07:42:15,484 - utils - INFO - Executing CrewAI workflow
2025-09-04 07:42:15,513 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:42:18,812 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:42:18,814 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:42:18,867 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:42:20,533 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:42:20,535 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:42:20,578 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:42:22,779 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:42:22,781 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:42:22,836 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:42:27,551 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:42:27,553 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:42:27,584 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:42:31,440 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:42:31,442 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:42:31,481 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:42:40,964 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:42:40,966 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:42:41,013 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:42:41,093 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-04 07:42:41,115 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11998, Requested 6154. Please try again in 30.756s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:42:41,115 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11998, Requested 6154. Please try again in 30.756s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:42:41,115 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11998, Requested 6154. Please try again in 30.756s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11998, Requested 6154. Please try again in 30.756s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:49:28,933 - __main__ - INFO - ==================================================
2025-09-04 07:49:28,933 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 07:49:28,933 - __main__ - INFO - Timestamp: 2025-09-04 07:49:28.933834
2025-09-04 07:49:28,933 - __main__ - INFO - ==================================================
2025-09-04 07:49:28,934 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 07:49:28,934 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 07:49:28,934 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 07:49:28,934 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 07:49:28,934 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 07:49:28,937 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-04 07:49:28,937 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 07:49:28,937 - utils - INFO - Starting daily market summary generation
2025-09-04 07:49:28,970 - utils - INFO - Step 1: Searching for financial news
2025-09-04 07:49:28,970 - utils - INFO - Step 2: Creating market summary
2025-09-04 07:49:28,970 - utils - INFO - Step 3: Formatting with visual elements
2025-09-04 07:49:28,970 - utils - INFO - Step 4: Translating to multiple languages
2025-09-04 07:49:28,971 - utils - INFO - Step 5: Sending to Telegram
2025-09-04 07:49:28,975 - utils - INFO - Executing CrewAI workflow
2025-09-04 07:49:29,000 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:49:31,624 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:49:31,627 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:49:31,667 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:49:32,959 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:49:32,960 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:49:32,994 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:49:34,506 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:49:34,508 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:49:34,538 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:49:38,778 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 07:49:38,780 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 07:49:38,817 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 07:49:38,893 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-04 07:49:38,916 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98994, Requested 1657. Please try again in 9m22.355s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:49:38,917 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98994, Requested 1657. Please try again in 9m22.355s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 07:49:38,917 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98994, Requested 1657. Please try again in 9m22.355s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 98994, Requested 1657. Please try again in 9m22.355s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:08:45,032 - __main__ - INFO - ==================================================
2025-09-04 18:08:45,033 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:08:45,033 - __main__ - INFO - Timestamp: 2025-09-04 18:08:45.033102
2025-09-04 18:08:45,033 - __main__ - INFO - ==================================================
2025-09-04 18:08:45,033 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:08:45,033 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:08:45,033 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:08:45,033 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:08:45,033 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:08:45,037 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-04 18:08:45,037 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:08:45,037 - utils - INFO - Starting daily market summary generation
2025-09-04 18:08:45,853 - utils - INFO - Step 1: Searching for financial news
2025-09-04 18:08:45,853 - utils - INFO - Step 2: Creating market summary
2025-09-04 18:08:45,854 - utils - INFO - Step 3: Formatting with visual elements
2025-09-04 18:08:45,854 - utils - INFO - Step 4: Translating to multiple languages
2025-09-04 18:08:45,854 - utils - INFO - Step 5: Sending to Telegram
2025-09-04 18:08:45,861 - utils - INFO - Executing CrewAI workflow
2025-09-04 18:08:45,903 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:08:48,567 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:08:48,570 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:08:48,613 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:08:50,243 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:08:50,245 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:08:50,291 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:08:51,991 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:08:51,993 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:08:52,038 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:08:56,316 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:08:56,318 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:08:56,359 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:09:00,097 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:09:00,098 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:09:00,125 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:09:10,974 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:09:10,975 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:09:11,011 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:09:11,108 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-04 18:09:11,188 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10741, Requested 6271. Please try again in 25.059s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:09:11,188 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10741, Requested 6271. Please try again in 25.059s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:09:11,188 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10741, Requested 6271. Please try again in 25.059s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10741, Requested 6271. Please try again in 25.059s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:12:36,616 - __main__ - INFO - ==================================================
2025-09-04 18:12:36,617 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:12:36,617 - __main__ - INFO - Timestamp: 2025-09-04 18:12:36.617165
2025-09-04 18:12:36,617 - __main__ - INFO - ==================================================
2025-09-04 18:12:36,617 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:12:36,617 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:12:36,617 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:12:36,617 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:12:36,617 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:12:36,621 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment to agents.
2025-09-04 18:12:36,621 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:12:36,621 - utils - INFO - Starting daily market summary generation
2025-09-04 18:12:36,621 - utils - INFO - Step 1: Searching for financial news
2025-09-04 18:12:36,621 - utils - INFO - Step 2: Creating market summary
2025-09-04 18:12:36,621 - utils - INFO - Step 3: Formatting with visual elements
2025-09-04 18:12:36,622 - utils - INFO - Step 4: Translating to multiple languages
2025-09-04 18:12:36,622 - utils - INFO - Step 5: Sending to Telegram
2025-09-04 18:12:36,626 - utils - INFO - Executing CrewAI workflow
2025-09-04 18:12:36,655 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:12:39,405 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:12:39,407 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:12:39,454 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:12:41,138 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:12:41,140 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:12:41,189 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:12:42,930 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:12:42,933 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:12:42,985 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:12:47,834 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:12:47,835 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:12:47,871 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:12:51,776 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:12:51,778 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:12:51,827 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:13:02,053 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:13:02,056 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:13:02,103 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:13:02,209 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-04 18:13:02,247 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10968, Requested 6339. Please try again in 26.531s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:13:02,247 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10968, Requested 6339. Please try again in 26.531s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:13:02,247 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10968, Requested 6339. Please try again in 26.531s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 40, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 117, in run_daily_summary
    result = crew.kickoff()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 10968, Requested 6339. Please try again in 26.531s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:20:18,165 - __main__ - INFO - ==================================================
2025-09-04 18:20:18,165 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:20:18,165 - __main__ - INFO - Timestamp: 2025-09-04 18:20:18.165326
2025-09-04 18:20:18,165 - __main__ - INFO - ==================================================
2025-09-04 18:20:18,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:20:18,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:20:18,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:20:18,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:20:18,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:20:18,855 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:20:18,855 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:20:18,855 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:20:18,855 - utils - INFO - Executing Search Task...
2025-09-04 18:20:18,855 - utils - ERROR - Error in daily summary workflow: 'Task' object has no attribute 'execute'
2025-09-04 18:20:18,855 - __main__ - ERROR - Market summary generation failed: 'Task' object has no attribute 'execute'
2025-09-04 18:20:18,855 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 54, in run_daily_summary
    search_result = search_task.execute(agent=self.search_agent)
                    ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\pydantic\main.py", line 991, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'Task' object has no attribute 'execute'
2025-09-04 18:21:52,369 - __main__ - INFO - ==================================================
2025-09-04 18:21:52,369 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:21:52,369 - __main__ - INFO - Timestamp: 2025-09-04 18:21:52.369396
2025-09-04 18:21:52,369 - __main__ - INFO - ==================================================
2025-09-04 18:21:53,036 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:21:53,036 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:21:53,037 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:21:53,037 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:21:53,037 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:21:53,041 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:21:53,042 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:21:53,042 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:21:53,042 - utils - INFO - Executing Search Task...
2025-09-04 18:21:53,070 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.3-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-04 18:21:53,070 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.3-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-04 18:21:53,071 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 54, in run_daily_summary
    search_result = search_task.execute_sync(agent=self.search_agent) # <-- CHANGED
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.3-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-04 18:23:09,486 - __main__ - INFO - ==================================================
2025-09-04 18:23:09,486 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:23:09,486 - __main__ - INFO - Timestamp: 2025-09-04 18:23:09.486541
2025-09-04 18:23:09,486 - __main__ - INFO - ==================================================
2025-09-04 18:23:10,149 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:23:10,149 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:23:10,149 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:23:10,149 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:23:10,149 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:23:10,154 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:23:10,154 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:23:10,154 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:23:10,154 - utils - INFO - Executing Search Task...
2025-09-04 18:23:10,168 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.3-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-04 18:23:10,168 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.3-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-04 18:23:10,169 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 54, in run_daily_summary
    search_result = search_task.execute_sync(agent=self.search_agent) # <-- CHANGED
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
    ...<5 lines>...
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1097, in completion
    model, custom_llm_provider, dynamic_api_key, api_base = get_llm_provider(
                                                            ~~~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<2 lines>...
        api_key=api_key,
        ^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 391, in get_llm_provider
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\get_llm_provider_logic.py", line 368, in get_llm_provider
    raise litellm.exceptions.BadRequestError(  # type: ignore
    ...<8 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=llama-3.3-70b-versatile
 Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers
2025-09-04 18:25:16,995 - __main__ - INFO - ==================================================
2025-09-04 18:25:16,995 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:25:16,995 - __main__ - INFO - Timestamp: 2025-09-04 18:25:16.995540
2025-09-04 18:25:16,995 - __main__ - INFO - ==================================================
2025-09-04 18:25:17,653 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:25:17,653 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:25:17,654 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:25:17,654 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:25:17,654 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:25:17,658 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:25:17,658 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:25:17,658 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:25:17,658 - utils - INFO - Executing Search Task...
2025-09-04 18:25:17,665 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:25:18,798 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:25:18,800 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:25:18,810 - utils - INFO - Search Task completed.
2025-09-04 18:25:43,810 - utils - INFO - Executing Summary Task...
2025-09-04 18:25:43,812 - utils - ERROR - Error in daily summary workflow: 1 validation error for TaskStartedEvent
context
  Input should be a valid string [type=string_type, input_value=TaskOutput(description='\...utputFormat.RAW: 'raw'>), input_type=TaskOutput]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-09-04 18:25:43,813 - __main__ - ERROR - Market summary generation failed: 1 validation error for TaskStartedEvent
context
  Input should be a valid string [type=string_type, input_value=TaskOutput(description='\...utputFormat.RAW: 'raw'>), input_type=TaskOutput]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-09-04 18:25:43,813 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 61, in run_daily_summary
    summary_result = summary_task.execute_sync(agent=self.summary_agent, context=search_result) # <-- CHANGED
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 445, in _execute_core
    crewai_event_bus.emit(self, TaskStartedEvent(context=context, task=self))
                                ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\events\task_events.py", line 15, in __init__
    super().__init__(**data)
    ~~~~~~~~~~~~~~~~^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\pydantic\main.py", line 253, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
pydantic_core._pydantic_core.ValidationError: 1 validation error for TaskStartedEvent
context
  Input should be a valid string [type=string_type, input_value=TaskOutput(description='\...utputFormat.RAW: 'raw'>), input_type=TaskOutput]
    For further information visit https://errors.pydantic.dev/2.11/v/string_type
2025-09-04 18:27:26,476 - __main__ - INFO - ==================================================
2025-09-04 18:27:26,476 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:27:26,476 - __main__ - INFO - Timestamp: 2025-09-04 18:27:26.476697
2025-09-04 18:27:26,476 - __main__ - INFO - ==================================================
2025-09-04 18:27:27,146 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:27:27,146 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:27:27,146 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:27:27,146 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:27:27,146 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:27:27,150 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:27:27,150 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:27:27,151 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:27:27,151 - utils - INFO - Executing Search Task...
2025-09-04 18:27:27,158 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:27:28,348 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:27:28,350 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:27:28,358 - utils - INFO - Search Task completed.
2025-09-04 18:27:53,359 - utils - INFO - Executing Summary Task...
2025-09-04 18:27:53,372 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:27:54,172 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:27:54,174 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:27:54,181 - utils - INFO - Summary Task completed.
2025-09-04 18:28:19,181 - utils - INFO - Executing Formatting Task...
2025-09-04 18:28:19,191 - LiteLLM - INFO - 
LiteLLM completion() model= llama3-8b-8192; provider = groq
2025-09-04 18:28:19,336 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-04 18:28:19,352 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:28:19,352 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:28:19,352 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 71, in run_daily_summary
    formatted_result = formatting_task.execute_sync(
        agent=self.formatting_agent,
        context=summary_result.raw  # <-- ADDED .raw
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 391, in exception_type
    raise BadRequestError(
    ...<6 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: GroqException - {"error":{"message":"The model `llama3-8b-8192` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:30:05,160 - __main__ - INFO - ==================================================
2025-09-04 18:30:05,160 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:30:05,160 - __main__ - INFO - Timestamp: 2025-09-04 18:30:05.160776
2025-09-04 18:30:05,160 - __main__ - INFO - ==================================================
2025-09-04 18:30:05,834 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:30:05,834 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:30:05,834 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:30:05,834 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:30:05,834 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:30:05,838 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:30:05,838 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:30:05,838 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:30:05,838 - utils - INFO - Executing Search Task...
2025-09-04 18:30:05,847 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:30:07,346 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:30:07,348 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:30:07,357 - utils - INFO - Search Task completed.
2025-09-04 18:30:32,358 - utils - INFO - Executing Summary Task...
2025-09-04 18:30:32,365 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:30:33,125 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:30:33,127 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:30:33,132 - utils - INFO - Summary Task completed.
2025-09-04 18:30:58,133 - utils - INFO - Executing Formatting Task...
2025-09-04 18:30:58,143 - LiteLLM - INFO - 
LiteLLM completion() model= llama3-groq-8b-8192-tool-use; provider = groq
2025-09-04 18:30:58,235 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-09-04 18:30:58,250 - utils - ERROR - Error in daily summary workflow: litellm.NotFoundError: GroqException - {"error":{"message":"The model `llama3-groq-8b-8192-tool-use` does not exist or you do not have access to it.","type":"invalid_request_error","code":"model_not_found"}}

2025-09-04 18:30:58,250 - __main__ - ERROR - Market summary generation failed: litellm.NotFoundError: GroqException - {"error":{"message":"The model `llama3-groq-8b-8192-tool-use` does not exist or you do not have access to it.","type":"invalid_request_error","code":"model_not_found"}}

2025-09-04 18:30:58,250 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"The model `llama3-groq-8b-8192-tool-use` does not exist or you do not have access to it.","type":"invalid_request_error","code":"model_not_found"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 71, in run_daily_summary
    formatted_result = formatting_task.execute_sync(
        agent=self.formatting_agent,
        context=summary_result.raw  # <-- ADDED .raw
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 349, in exception_type
    raise NotFoundError(
    ...<5 lines>...
    )
litellm.exceptions.NotFoundError: litellm.NotFoundError: GroqException - {"error":{"message":"The model `llama3-groq-8b-8192-tool-use` does not exist or you do not have access to it.","type":"invalid_request_error","code":"model_not_found"}}

2025-09-04 18:33:33,179 - __main__ - INFO - ==================================================
2025-09-04 18:33:33,179 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:33:33,180 - __main__ - INFO - Timestamp: 2025-09-04 18:33:33.180066
2025-09-04 18:33:33,180 - __main__ - INFO - ==================================================
2025-09-04 18:33:33,849 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:33:33,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:33:33,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:33:33,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:33:33,850 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:33:33,854 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:33:33,854 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:33:33,854 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:33:33,854 - utils - INFO - Executing Search Task...
2025-09-04 18:33:33,861 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:33:34,980 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:33:34,983 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:33:34,990 - utils - INFO - Search Task completed.
2025-09-04 18:33:59,991 - utils - INFO - Executing Summary Task...
2025-09-04 18:34:00,005 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:34:00,575 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:34:00,578 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:34:00,586 - utils - INFO - Summary Task completed.
2025-09-04 18:34:25,586 - utils - INFO - Executing Formatting Task...
2025-09-04 18:34:25,598 - LiteLLM - INFO - 
LiteLLM completion() model= gemma-7b-it; provider = groq
2025-09-04 18:34:25,696 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-04 18:34:25,711 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: GroqException - {"error":{"message":"The model `gemma-7b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:34:25,711 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: GroqException - {"error":{"message":"The model `gemma-7b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:34:25,711 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"The model `gemma-7b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 71, in run_daily_summary
    formatted_result = formatting_task.execute_sync(
        agent=self.formatting_agent,
        context=summary_result.raw  # <-- ADDED .raw
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 391, in exception_type
    raise BadRequestError(
    ...<6 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: GroqException - {"error":{"message":"The model `gemma-7b-it` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:36:41,810 - __main__ - INFO - ==================================================
2025-09-04 18:36:41,810 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:36:41,810 - __main__ - INFO - Timestamp: 2025-09-04 18:36:41.810508
2025-09-04 18:36:41,810 - __main__ - INFO - ==================================================
2025-09-04 18:36:42,467 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:36:42,467 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:36:42,467 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:36:42,467 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:36:42,467 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:36:42,471 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:36:42,471 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:36:42,472 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:36:42,472 - utils - INFO - Executing Search Task...
2025-09-04 18:36:42,479 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:36:43,717 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:36:43,719 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:36:43,728 - utils - INFO - Search Task completed.
2025-09-04 18:37:08,728 - utils - INFO - Executing Summary Task...
2025-09-04 18:37:08,739 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:37:09,407 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:37:09,409 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:37:09,416 - utils - INFO - Summary Task completed.
2025-09-04 18:37:34,417 - utils - INFO - Executing Formatting Task...
2025-09-04 18:37:34,423 - LiteLLM - INFO - 
LiteLLM completion() model= mixtral-8x7b-32768; provider = groq
2025-09-04 18:37:34,533 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-09-04 18:37:34,551 - utils - ERROR - Error in daily summary workflow: litellm.BadRequestError: GroqException - {"error":{"message":"The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:37:34,551 - __main__ - ERROR - Market summary generation failed: litellm.BadRequestError: GroqException - {"error":{"message":"The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:37:34,551 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '400 Bad Request' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 71, in run_daily_summary
    formatted_result = formatting_task.execute_sync(
        agent=self.formatting_agent,
        context=summary_result.raw  # <-- ADDED .raw
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 391, in exception_type
    raise BadRequestError(
    ...<6 lines>...
    )
litellm.exceptions.BadRequestError: litellm.BadRequestError: GroqException - {"error":{"message":"The model `mixtral-8x7b-32768` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.","type":"invalid_request_error","code":"model_decommissioned"}}

2025-09-04 18:38:43,848 - __main__ - INFO - ==================================================
2025-09-04 18:38:43,848 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:38:43,848 - __main__ - INFO - Timestamp: 2025-09-04 18:38:43.848448
2025-09-04 18:38:43,848 - __main__ - INFO - ==================================================
2025-09-04 18:38:44,208 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans.ttf
2025-09-04 18:38:44,208 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf
2025-09-04 18:38:44,208 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansArabic-Regular.ttf
2025-09-04 18:38:44,208 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansHebrew-Regular.ttf
2025-09-04 18:38:44,208 - pdf_generator - WARNING - Font not found: /usr/share/fonts/truetype/noto/NotoSansDevanagari-Regular.ttf
2025-09-04 18:38:44,212 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:38:44,212 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:38:44,212 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:38:44,212 - utils - INFO - Executing Search Task...
2025-09-04 18:38:44,220 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:38:45,409 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:38:45,411 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:38:45,418 - utils - INFO - Search Task completed.
2025-09-04 18:39:10,419 - utils - INFO - Executing Summary Task...
2025-09-04 18:39:10,430 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:39:11,001 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:39:11,004 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:39:11,011 - utils - INFO - Summary Task completed.
2025-09-04 18:39:36,012 - utils - INFO - Executing Formatting Task...
2025-09-04 18:39:36,023 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:39:36,751 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:39:36,753 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:39:36,758 - utils - INFO - Formatting Task completed.
2025-09-04 18:40:01,759 - utils - INFO - Executing Translation Task for: HI
2025-09-04 18:40:01,771 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:40:03,295 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:40:03,298 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:40:03,307 - utils - INFO - Translation to HI completed.
2025-09-04 18:40:28,308 - utils - INFO - Executing Translation Task for: AR
2025-09-04 18:40:28,317 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:40:29,728 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:40:29,730 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:40:29,740 - utils - INFO - Translation to AR completed.
2025-09-04 18:40:54,740 - utils - INFO - Executing Translation Task for: HE
2025-09-04 18:40:54,751 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:40:57,916 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:40:57,918 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:40:57,925 - utils - INFO - Translation to HE completed.
2025-09-04 18:40:57,925 - utils - INFO - Generating PDF output
2025-09-04 18:40:57,950 - pdf_generator - ERROR - PDF generation failed: format not resolved, probably missing URL scheme or undefined destination target for 'english'
2025-09-04 18:40:57,950 - utils - ERROR - PDF generation failed: format not resolved, probably missing URL scheme or undefined destination target for 'english'
2025-09-04 18:40:57,950 - utils - ERROR - Error in daily summary workflow: format not resolved, probably missing URL scheme or undefined destination target for 'english'
2025-09-04 18:40:57,950 - __main__ - ERROR - Market summary generation failed: format not resolved, probably missing URL scheme or undefined destination target for 'english'
2025-09-04 18:40:57,950 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 94, in run_daily_summary
    self.generate_pdf_output(translations)
    ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 106, in generate_pdf_output
    pdf_path = self.pdf_generator.generate_pdf(all_translations)
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\pdf_generator.py", line 275, in generate_pdf
    doc.build(story)
    ~~~~~~~~~^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\platypus\doctemplate.py", line 1322, in build
    BaseDocTemplate.build(self,flowables, canvasmaker=canvasmaker)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\platypus\doctemplate.py", line 1109, in build
    self._endBuild()
    ~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\platypus\doctemplate.py", line 1044, in _endBuild
    if getattr(self,'_doSave',1): self.canv.save()
                                  ~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfgen\canvas.py", line 1301, in save
    self._doc.SaveToFile(self._filename, self)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 212, in SaveToFile
    data = self.GetPDFData(canvas)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 238, in GetPDFData
    return self.format()
           ~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 420, in format
    IOf = IO.format(self)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 867, in format
    fcontent = format(self.content, document, toplevel=1)   # yes this is at top level
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 67, in format
    f = element.format(document)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 1633, in format
    return D.format(document)
           ~~~~~~~~^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 680, in format
    L = [(format(PDFName(k),document)+b" "+format(dict[k],document)) for k in keys]
                                           ~~~~~~^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 67, in format
    f = element.format(document)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\pdfdoc.py", line 1797, in format
    if f is None: raise ValueError("format not resolved, probably missing URL scheme or undefined destination target for '%s'" % self.name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: format not resolved, probably missing URL scheme or undefined destination target for 'english'
2025-09-04 18:45:27,214 - __main__ - INFO - ==================================================
2025-09-04 18:45:27,215 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:45:27,215 - __main__ - INFO - Timestamp: 2025-09-04 18:45:27.215211
2025-09-04 18:45:27,215 - __main__ - INFO - ==================================================
2025-09-04 18:45:27,585 - __main__ - ERROR - Market summary generation failed: Can't open file "Helvetica.afm"
2025-09-04 18:45:27,585 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\lib\utils.py", line 523, in open_for_read
    return open_for_read_by_name(name,mode)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\lib\utils.py", line 463, in open_for_read_by_name
    return open(name,mode)
FileNotFoundError: [Errno 2] No such file or directory: 'Helvetica.afm'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\lib\utils.py", line 530, in open_for_read
    return BytesIO((datareader if name[:5].lower()=='data:' else rlUrlRead)(name))
                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\lib\utils.py", line 476, in rlUrlRead
    return urlopen(name).read()
           ~~~~~~~^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\urllib\request.py", line 189, in urlopen
    return opener.open(url, data, timeout)
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\urllib\request.py", line 473, in open
    req = Request(fullurl, data)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\urllib\request.py", line 292, in __init__
    self.full_url = url
    ^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\urllib\request.py", line 318, in full_url
    self._parse()
    ~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\urllib\request.py", line 347, in _parse
    raise ValueError("unknown url type: %r" % self.full_url)
ValueError: unknown url type: 'Helvetica.afm'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\ttfonts.py", line 159, in TTFOpenFile
    f = open_for_read(fn,'rb')
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\lib\utils.py", line 534, in open_for_read
    return open_for_read(name,mode)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\lib\utils.py", line 532, in open_for_read
    raise IOError('Cannot open resource "%s"' % name)
OSError: Cannot open resource "Helvetica.afm"

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 32, in run_summary
    crew = MarketSummaryCrew()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 26, in __init__
    self.pdf_generator = PDFGenerator()
                         ~~~~~~~~~~~~^^
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\pdf_generator.py", line 32, in __init__
    self._register_fonts()
    ~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\pdf_generator.py", line 44, in _register_fonts
    pdfmetrics.registerFont(TTFont('Helvetica', 'Helvetica.afm'))
                            ~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\ttfonts.py", line 1207, in __init__
    self.face = TTFontFace(filename, validate=validate, subfontIndex=subfontIndex)
                ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\ttfonts.py", line 1088, in __init__
    TTFontFile.__init__(self, filename, validate=validate, subfontIndex=subfontIndex)
    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\ttfonts.py", line 447, in __init__
    TTFontParser.__init__(self, file, validate=validate,subfontIndex=subfontIndex)
    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\ttfonts.py", line 183, in __init__
    self.readFile(file)
    ~~~~~~~~~~~~~^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\ttfonts.py", line 259, in readFile
    self.filename, f = TTFOpenFile(f)
                       ~~~~~~~~~~~^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\reportlab\pdfbase\ttfonts.py", line 169, in TTFOpenFile
    raise TTFError('Can\'t open file "%s"' % fn)
reportlab.pdfbase.ttfonts.TTFError: Can't open file "Helvetica.afm"
2025-09-04 18:46:58,938 - __main__ - INFO - ==================================================
2025-09-04 18:46:58,938 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:46:58,938 - __main__ - INFO - Timestamp: 2025-09-04 18:46:58.938329
2025-09-04 18:46:58,938 - __main__ - INFO - ==================================================
2025-09-04 18:46:59,295 - pdf_generator - WARNING - Font not found: fonts/NotoSansArabic-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Arabic language support in the PDF.
2025-09-04 18:46:59,295 - pdf_generator - WARNING - Font not found: fonts/NotoSansHebrew-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Hebrew language support in the PDF.
2025-09-04 18:46:59,295 - pdf_generator - WARNING - Font not found: fonts/NotoSansDevanagari-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Devanagari language support in the PDF.
2025-09-04 18:46:59,295 - __main__ - ERROR - Market summary generation failed: name 'getSampleStyleStyleSheet' is not defined
2025-09-04 18:46:59,296 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 32, in run_summary
    crew = MarketSummaryCrew()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 26, in __init__
    self.pdf_generator = PDFGenerator()
                         ~~~~~~~~~~~~^^
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\pdf_generator.py", line 33, in __init__
    self.styles = self._create_styles()
                  ~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\pdf_generator.py", line 61, in _create_styles
    styles = getSampleStyleStyleSheet()
             ^^^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'getSampleStyleStyleSheet' is not defined. Did you mean: 'getSampleStyleSheet'?
2025-09-04 18:48:22,750 - __main__ - INFO - ==================================================
2025-09-04 18:48:22,750 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:48:22,750 - __main__ - INFO - Timestamp: 2025-09-04 18:48:22.750907
2025-09-04 18:48:22,751 - __main__ - INFO - ==================================================
2025-09-04 18:48:23,108 - pdf_generator - WARNING - Font not found: fonts/NotoSansArabic-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Arabic language support in the PDF.
2025-09-04 18:48:23,108 - pdf_generator - WARNING - Font not found: fonts/NotoSansHebrew-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Hebrew language support in the PDF.
2025-09-04 18:48:23,108 - pdf_generator - WARNING - Font not found: fonts/NotoSansDevanagari-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Devanagari language support in the PDF.
2025-09-04 18:48:23,112 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:48:23,112 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:48:23,113 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:48:23,113 - utils - INFO - Executing Search Task...
2025-09-04 18:48:23,120 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:48:24,243 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:48:24,246 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:48:24,256 - utils - INFO - Search Task completed.
2025-09-04 18:48:49,257 - utils - INFO - Executing Summary Task...
2025-09-04 18:48:49,269 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:48:50,059 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:48:50,061 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:48:50,068 - utils - INFO - Summary Task completed.
2025-09-04 18:49:15,069 - utils - INFO - Executing Formatting Task...
2025-09-04 18:49:15,077 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:49:15,746 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:49:15,748 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:49:15,754 - utils - INFO - Formatting Task completed.
2025-09-04 18:49:40,755 - utils - INFO - Executing Translation Task for: HI
2025-09-04 18:49:40,765 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:49:42,604 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:49:42,606 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:49:42,613 - utils - INFO - Translation to HI completed.
2025-09-04 18:50:07,614 - utils - INFO - Executing Translation Task for: AR
2025-09-04 18:50:07,624 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:50:09,248 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:50:09,250 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:50:09,258 - utils - INFO - Translation to AR completed.
2025-09-04 18:50:34,258 - utils - INFO - Executing Translation Task for: HE
2025-09-04 18:50:34,270 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:50:38,826 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-09-04 18:50:38,828 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-09-04 18:50:38,835 - utils - INFO - Translation to HE completed.
2025-09-04 18:50:38,835 - utils - INFO - Generating PDF output
2025-09-04 18:50:40,044 - pdf_generator - ERROR - Could not fetch or process image from https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png: 403 Client Error: Forbidden for url: https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png
2025-09-04 18:50:40,452 - pdf_generator - ERROR - Could not fetch or process image from https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png: 403 Client Error: Forbidden for url: https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png
2025-09-04 18:50:40,765 - pdf_generator - ERROR - Could not fetch or process image from https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png: 403 Client Error: Forbidden for url: https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png
2025-09-04 18:50:40,987 - pdf_generator - ERROR - Could not fetch or process image from https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png: 403 Client Error: Forbidden for url: https://cdn.pixabay.com/photo/2016/08/25/10/49/bear-1617785_1280.png
2025-09-04 18:50:41,008 - pdf_generator - INFO - Successfully generated PDF: output\market_summary_2025-09-04.pdf
2025-09-04 18:50:41,008 - utils - INFO - PDF generated: output\market_summary_2025-09-04.pdf
2025-09-04 18:50:41,008 - utils - INFO - Daily market summary workflow finished successfully.
2025-09-04 18:50:41,008 - __main__ - INFO - Market summary generation completed successfully
2025-09-04 18:50:41,029 - utils - INFO - Cleanup completed
2025-09-04 18:55:13,462 - __main__ - INFO - ==================================================
2025-09-04 18:55:13,463 - __main__ - INFO - Starting Daily Market Summary Generation
2025-09-04 18:55:13,463 - __main__ - INFO - Timestamp: 2025-09-04 18:55:13.463119
2025-09-04 18:55:13,463 - __main__ - INFO - ==================================================
2025-09-04 18:55:13,821 - pdf_generator - WARNING - Font not found: fonts/NotoSansArabic-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Arabic language support in the PDF.
2025-09-04 18:55:13,821 - pdf_generator - WARNING - Font not found: fonts/NotoSansHebrew-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Hebrew language support in the PDF.
2025-09-04 18:55:13,821 - pdf_generator - WARNING - Font not found: fonts/NotoSansDevanagari-Regular.ttf. Please download it and place it in the 'fonts' directory for proper Devanagari language support in the PDF.
2025-09-04 18:55:13,825 - utils - WARNING - crewai_tools.BaseTool not available; skipping tool attachment.
2025-09-04 18:55:13,825 - utils - INFO - Market Summary Crew initialized successfully
2025-09-04 18:55:13,825 - utils - INFO - Starting manual task execution to avoid rate limits.
2025-09-04 18:55:13,826 - utils - INFO - Executing Search Task...
2025-09-04 18:55:13,832 - LiteLLM - INFO - 
LiteLLM completion() model= llama-3.3-70b-versatile; provider = groq
2025-09-04 18:55:14,234 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-09-04 18:55:14,245 - utils - ERROR - Error in daily summary workflow: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100362, Requested 402. Please try again in 11m0.315s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:55:14,245 - __main__ - ERROR - Market summary generation failed: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100362, Requested 402. Please try again in 11m0.315s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

2025-09-04 18:55:14,246 - __main__ - ERROR - Full traceback:
Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 1824, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100362, Requested 402. Please try again in 11m0.315s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\run_market_summary.py", line 33, in run_summary
    result = crew.run_daily_summary()
  File "C:\Users\AI\Desktop\CrowdWisdomTrading AI Agent\market_summary_crew.py", line 54, in run_daily_summary
    search_result = search_task.execute_sync(agent=self.search_agent)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 382, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 530, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\task.py", line 446, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 475, in execute_task
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 451, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agent.py", line 547, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 1030, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\AI\AppData\Local\Programs\Python\Python313\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 329, in exception_type
    raise RateLimitError(
    ...<4 lines>...
    )
litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {"error":{"message":"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k470b6fffwhbgbkgj9nqc0cz` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100362, Requested 402. Please try again in 11m0.315s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}

